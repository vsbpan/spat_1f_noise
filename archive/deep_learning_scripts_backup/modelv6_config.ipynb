{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c9ac50-09d3-4518-93f7-7cd57391e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:56:38_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n",
      "torch:  2.1 ; cuda:  2.1.1\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50920f3-30b3-41fd-b145-649646616d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor,DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.visualizer import ColorMode,GenericMask,Visualizer\n",
    "from detectron2.structures.keypoints import heatmaps_to_keypoints\n",
    "from detectron2.structures.keypoints_hack import heatmaps_to_keypoints2, heatmaps_to_keypoints_iterative, heatmaps_modify_iterative\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    DatasetEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    "    verify_results,\n",
    ")\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf00594-8f23-4c1b-b009-2ea4ffc964b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\R_projects\\deep_learning_playground\\detectron2\\custom_data3\n"
     ]
    }
   ],
   "source": [
    "cd C:\\R_projects\\deep_learning_playground\\detectron2\\custom_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63001ee9-b8f6-4467-8d3a-054f6fd614fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='cat_data_test',\n",
       "          json_file='.\\\\coco_test.json',\n",
       "          image_root='.\\\\datasets\\\\sample1',\n",
       "          evaluator_type='coco',\n",
       "          thing_classes=['cat'],\n",
       "          thing_colors=(0, 0, 255),\n",
       "          keypoint_names=['head', 'middle', 'tail'],\n",
       "          keypoint_flip_map=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_coco_instances(\"cat_data_train\", {}, \".\\coco_train.json\", \".\\datasets\\sample1\")\n",
    "register_coco_instances(\"cat_data_test\", {}, \".\\coco_test.json\", \".\\datasets\\sample1\")\n",
    "#register_coco_instances(\"cat_data_val\", {}, \".\\valid.json\", \".\\datasets\\caterpillar\")\n",
    "MetadataCatalog.get(\"cat_data_train\").set(thing_classes=[\"cat\"])\n",
    "MetadataCatalog.get(\"cat_data_train\").set(thing_colors=(0,0,255))\n",
    "MetadataCatalog.get(\"cat_data_test\").set(thing_classes=[\"cat\"])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(thing_colors=(0,0,255))\n",
    "MetadataCatalog.get(\"cat_data_train\").set(keypoint_names=[\"head\",\"middle\",\"tail\"])\n",
    "MetadataCatalog.get(\"cat_data_train\").set(keypoint_flip_map=[])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(keypoint_names=[\"head\",\"middle\",\"tail\"])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(keypoint_flip_map=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d7da83-2d06-4f19-a8c5-5d4767d53900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer deriving from the \"DefaultTrainer\"\n",
    "\n",
    "    Overloads build_hooks to add a hook to calculate loss on the test set during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1, LossEvalHook(\n",
    "            400, # Frequency of calculation - every 100 iterations here\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg, True)\n",
    "            )\n",
    "        ))\n",
    "\n",
    "        return hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa0596-8d92-4cf1-b5be-ddd8f8435a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b324ae2-5000-4056-a4d6-1fbe71836476",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"cat_data_train\",)\n",
    "cfg.DATASETS.TEST = (\"cat_data_test\", )\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.TEST.EVAL_PERIOD = 500\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.0001  \n",
    "cfg.SOLVER.MAX_ITER = 8000    \n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.03 # Set to lower?\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 3\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.1, 0.3, 0.1]\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.MODEL.MASK_ON = True\n",
    "cfg.MODEL.KEYPOINT_ON = True\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT = 5\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS = False\n",
    "#cfg.MODEL.ROI_KEYPOINT_HEAD.ITERATIVE = True\n",
    "cfg.INPUT.USE_DIFF = False                      # Include difference in green channel 10 frames ago as the 4th channel\n",
    "cfg.OUTPUT_DIR = \"./modelv6\"\n",
    "#cfg.MODEL.BACKBONE.FREEZE_AT = 0\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1000\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1000\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 1000\n",
    "\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 5000\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 3000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 2500\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 2000\n",
    "cfg.MODEL.RPN.NMS_THRESH = 0.7\n",
    "\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9070c7-48c5-4adc-834d-d7f2a9231914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: True\n",
      "  NUM_WORKERS: 2\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ('cat_data_test',)\n",
      "  TRAIN: ('cat_data_train',)\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1000\n",
      "  MAX_SIZE_TRAIN: 1000\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: 1000\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "  USE_DIFF: False\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32], [64], [128], [256], [512]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_fpn_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: True\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_ON: True\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
      "    FED_LOSS_NUM_CLASSES: 50\n",
      "    NAME: FastRCNNConvFCHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 2\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "    USE_FED_LOSS: False\n",
      "    USE_SIGMOID_CE: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 128\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: StandardROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 1\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.03\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    ITERATIVE: True\n",
      "    LOSS_WEIGHT: 5\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: False\n",
      "    NUM_KEYPOINTS: 3\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS: [-1]\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 2000\n",
      "    POST_NMS_TOPK_TRAIN: 2500\n",
      "    PRE_NMS_TOPK_TEST: 3000\n",
      "    PRE_NMS_TOPK_TRAIN: 5000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
      "OUTPUT_DIR: ./modelv6\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BASE_LR: 0.0001\n",
      "  BASE_LR_END: 0.0\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 500\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 2\n",
      "  LR_SCHEDULER_NAME: WarmupCosineLR\n",
      "  MAX_ITER: 8000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  NUM_DECAYS: 3\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  RESCALE_INTERVAL: False\n",
      "  STEPS: (210000, 250000)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: None\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 1\n",
      "  EVAL_PERIOD: 500\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: [0.1, 0.3, 0.1]\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88454e6c-5124-4597-8c6c-8958e84a2336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1d8f552bcda2bd1c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1d8f552bcda2bd1c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir cfg.OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9a0080d-f0ce-43ef-8c35-16e32d7b2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/18 14:31:10 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./modelv6\\model_0007999.pth ...\n"
     ]
    }
   ],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_0007999.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28ed2e-206b-4dab-b6d2-46315a5ec43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cffa479-d462-4606-9854-97c53546b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For writing ground truths and predictions \n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def write_csv(path, df):\n",
    "    keys = df[0].keys()\n",
    "    with open(path, \"w\", newline = \"\") as output_file: \n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(df)\n",
    "\n",
    "def extract_pred_annotations(predictor, fi, data_name):\n",
    "    inst = predictor(cv2.imread(fi))[\"instances\"].to(\"cpu\")\n",
    "    data = {}\n",
    "    data[\"file_name\"] = fi\n",
    "    data[\"image_size\"] = list(inst.image_size)\n",
    "    try: data[\"thing_class\"] = MetadataCatalog.get(data_name).get(\"thing_classes\")[inst.pred_classes[0]]\n",
    "    except: data[\"thing_class\"] = \"NA\"\n",
    "    try: data[\"score\"] = inst.scores.tolist()[0]\n",
    "    except: data[\"score\"] = \"NA\"\n",
    "    try: data[\"keypoints\"] = np.array(inst.pred_keypoints.tolist()[0]).ravel().tolist()\n",
    "    except: data[\"keypoints\"] = \"NA\"\n",
    "    try: data[\"bbox\"] = inst.pred_boxes.tensor.tolist()[0]\n",
    "    except: data[\"bbox\"] = \"NA\"\n",
    "    try:\n",
    "        cont, _ = cv2.findContours(inst.pred_masks.numpy()[0,:,:].astype('uint8'),cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "        data[\"polygon\"] = cont[0].ravel().tolist()\n",
    "    except: \n",
    "        data[\"polygon\"] = \"NA\"\n",
    "    data[\"type\"] = \"prediction\"\n",
    "    return(data)\n",
    "\n",
    "def extract_gt_annotations(data_dict, data_name):\n",
    "    data = {}\n",
    "    data[\"file_name\"] = data_dict[\"file_name\"]\n",
    "    data[\"image_size\"] = [data_dict[\"width\"], data_dict[\"height\"]]\n",
    "    \n",
    "    ann = data_dict[\"annotations\"][0] # One instance for now\n",
    "    \n",
    "    try: data[\"thing_class\"] = MetadataCatalog.get('cat_data_test').get(\"thing_classes\")[ann[\"category_id\"]]\n",
    "    except: data[\"thing_class\"] = \"NA\"\n",
    "    data[\"score\"] = \"NA\"\n",
    "    try: data[\"keypoints\"] = ann[\"keypoints\"]\n",
    "    except: data[\"keypoints\"] = \"NA\"\n",
    "    try: data[\"bbox\"] = ann[\"bbox\"]\n",
    "    except: data[\"bbox\"] = \"NA\"\n",
    "    try: data[\"polygon\"] = ann[\"segmentation\"][0]\n",
    "    except: data[\"polygon\"] = \"NA\"\n",
    "    data[\"type\"] = \"ground_truth\"\n",
    "    return(data)\n",
    "    \n",
    "\n",
    "def img_inference2(predictor, data_name, inference_info = \"\"):\n",
    "    d = DatasetCatalog.get(data_name)\n",
    "    dataset = []\n",
    "    tot = len(d)\n",
    "    for i in range(tot):\n",
    "        print(f\"{i+1} of {tot}\", end = \"\\r\")\n",
    "        fi = d[i][\"file_name\"]\n",
    "        data = extract_pred_annotations(predictor, fi, data_name)\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "        data = extract_gt_annotations(d[i], data_name)\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "    return(dataset)\n",
    "\n",
    "def write_img_inference2(predictor, write_path, file_name_ID, data_name, model_ver):\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    inference_info = model_ver + \"__\" + now\n",
    "    df = img_inference2(predictor, data_name, inference_info)\n",
    "    write_csv(os.path.join(write_path, file_name_ID + \"_inference.csv\"), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "febbc83b-8d2e-4801-83ec-56f35e0e4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For writing predictions\n",
    "import csv\n",
    "\n",
    "def write_csv(path, df):\n",
    "    keys = df[0].keys()\n",
    "    with open(path, \"w\", newline = \"\") as output_file: \n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(df)\n",
    "\n",
    "def img_inference(predictor, rep_ID, root_path, inference_info):\n",
    "    dataset = []\n",
    "    f = glob.glob(os.path.join(root_path, rep_ID,\"*[!_diff].jpg\"))\n",
    "    tot = len(f)\n",
    "    for i in range(tot):\n",
    "        print(f\"{i+1} of {tot}\", end = \"\\r\")\n",
    "        fi = f[i]\n",
    "        inst = predictor(cv2.imread(fi))[\"instances\"].to(\"cpu\")\n",
    "        data = {}\n",
    "        data[\"file_name\"] = fi\n",
    "        data[\"image_size\"] = list(inst.image_size)\n",
    "        try: data[\"thing_class\"] = MetadataCatalog.get('cat_data_test').get(\"thing_classes\")[inst.pred_classes[0]]\n",
    "        except: data[\"thing_class\"] = \"NA\"\n",
    "        try: data[\"score\"] = inst.scores.tolist()[0]\n",
    "        except: data[\"score\"] = \"NA\"\n",
    "        try: data[\"keypoints\"] = np.array(inst.pred_keypoints.tolist()[0]).ravel().tolist()\n",
    "        except: data[\"keypoints\"] = \"NA\"\n",
    "        try: data[\"bbox\"] = inst.pred_boxes.tensor.tolist()[0]\n",
    "        except: data[\"bbox\"] = \"NA\"\n",
    "        try:\n",
    "            cont, _ = cv2.findContours(inst.pred_masks.numpy()[0,:,:].astype('uint8'),cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "            data[\"polygon\"] = cont[0].ravel().tolist()\n",
    "        except: \n",
    "            data[\"polygon\"] = \"NA\"\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "    return(dataset)\n",
    "\n",
    "def write_img_inference(predictor, write_path, rep_ID, read_path, model_ver):\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    inference_info = model_ver + \"__\" + now\n",
    "    df = img_inference(predictor, rep_ID, read_path, inference_info)\n",
    "    write_csv(os.path.join(write_path, rep_ID + \"_inference.csv\"), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143bc63-e509-43b4-bed4-4c1cad41715f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b0d9f-f8ab-4ef6-a6b7-98ada41e6ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d67f93-cc9e-4df0-8b52-bece7b173353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eff8fde-a3c4-46b7-a4db-b02b1e49754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/18 14:31:17 d2.data.datasets.coco]: \u001b[0mLoaded 123 images in COCO format from .\\coco_test.json\n",
      "1 of 123\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vsbpa\\.conda\\envs\\detectron_env\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 of 123\r"
     ]
    }
   ],
   "source": [
    "write_img_inference2(\n",
    "    predictor, \n",
    "    write_path = \"C:/R_projects/deep_learning_playground\",\n",
    "    file_name_ID = \"sample1_modelv6\",\n",
    "    data_name = \"cat_data_test\", \n",
    "    model_ver = \"modelv6\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd13b996-4d7c-40cc-a27d-d49dcadd8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_IDs = [os.path.basename(x) \n",
    " for x in \n",
    " glob.glob(\"C:/R_projects/spat_1f_noise/processed_feed/rep*\", recursive=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14f5479-af8c-4dc5-94d9-4d33b3d39226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rep1\n",
      "Processing rep10\n",
      "Processing rep100\n",
      "Processing rep101\n",
      "Processing rep102\n",
      "Processing rep103\n",
      "Processing rep104\n",
      "Processing rep105\n",
      "Processing rep106\n",
      "Processing rep107\n",
      "Processing rep108\n",
      "Processing rep109\n",
      "Processing rep11\n",
      "Processing rep110\n",
      "Processing rep111\n",
      "Processing rep112\n",
      "Processing rep113\n",
      "Processing rep114\n",
      "Processing rep115\n",
      "Processing rep116\n",
      "Processing rep117\n",
      "Processing rep118\n",
      "Processing rep119\n",
      "Processing rep12\n",
      "Processing rep120\n",
      "Processing rep121\n",
      "Processing rep129\n",
      "Processing rep13\n",
      "Processing rep130\n",
      "Processing rep131\n",
      "Processing rep132\n",
      "Processing rep133\n",
      "Processing rep134\n",
      "Processing rep135\n",
      "Processing rep136\n",
      "Processing rep137\n",
      "Processing rep138\n",
      "Processing rep139\n",
      "Processing rep14\n",
      "Processing rep140\n",
      "Processing rep141\n",
      "Processing rep142\n",
      "Processing rep143\n",
      "Processing rep144\n",
      "Processing rep145\n",
      "Processing rep146\n",
      "Processing rep147\n",
      "Processing rep148\n",
      "Processing rep149\n",
      "Processing rep15\n",
      "Processing rep150\n",
      "Processing rep151\n",
      "Processing rep152\n",
      "Processing rep153\n",
      "Processing rep154\n",
      "Processing rep155\n",
      "Processing rep156\n",
      "Processing rep157\n",
      "Processing rep158\n",
      "Processing rep16\n",
      "Processing rep17\n",
      "Processing rep18\n",
      "Processing rep19\n",
      "Processing rep2\n",
      "Processing rep20\n",
      "Processing rep21\n",
      "Processing rep22\n",
      "Processing rep23\n",
      "Processing rep24\n",
      "Processing rep25\n",
      "Processing rep26\n",
      "Processing rep27\n",
      "Processing rep28\n",
      "Processing rep29\n",
      "Processing rep3\n",
      "Processing rep30\n",
      "Processing rep31\n",
      "Processing rep32\n",
      "Processing rep33\n",
      "Processing rep34\n",
      "Processing rep35\n",
      "Processing rep36\n",
      "Processing rep37\n",
      "Processing rep38\n",
      "Processing rep39\n",
      "Processing rep4\n",
      "Processing rep40\n",
      "Processing rep41\n",
      "Processing rep42\n",
      "Processing rep44\n",
      "Processing rep45\n",
      "Processing rep46\n",
      "Processing rep47\n",
      "Processing rep48\n",
      "Processing rep49\n",
      "Processing rep5\n",
      "Processing rep50\n",
      "Processing rep51\n",
      "Processing rep52\n",
      "Processing rep53\n",
      "Processing rep54\n",
      "Processing rep55\n",
      "Processing rep56\n",
      "Processing rep57\n",
      "Processing rep58\n",
      "Processing rep59\n",
      "Processing rep6\n",
      "Processing rep60\n",
      "Processing rep61\n",
      "Processing rep62\n",
      "Processing rep63\n",
      "Processing rep64\n",
      "Processing rep65\n",
      "Processing rep66\n",
      "Processing rep67\n",
      "Processing rep68\n",
      "Processing rep69\n",
      "Processing rep7\n",
      "Processing rep70\n",
      "Processing rep71\n",
      "Processing rep72\n",
      "Processing rep73\n",
      "Processing rep74\n",
      "Processing rep75\n",
      "Processing rep77\n",
      "Processing rep78\n",
      "Processing rep79\n",
      "Processing rep8\n",
      "Processing rep80\n",
      "Processing rep81\n",
      "Processing rep82\n",
      "Processing rep83\n",
      "Processing rep84\n",
      "Processing rep85\n",
      "Processing rep86\n",
      "Processing rep87\n",
      "Processing rep88\n",
      "Processing rep89\n",
      "Processing rep9\n",
      "Processing rep90\n",
      "Processing rep91\n",
      "Processing rep92\n",
      "Processing rep93\n",
      "Processing rep94\n",
      "Processing rep95\n",
      "Processing rep96\n",
      "Processing rep97\n",
      "Processing rep98\n",
      "Processing rep99\n",
      "Processing reptrial_2\n",
      "Processing reptrial_3\n",
      "Processing reptrial_4\n",
      "Processing reptrial_5\n",
      "Processing reptrial_6\n",
      "Processing repweek2test\n",
      "Processing repweek2test2\n",
      "Processing repweek2test3\n",
      "Processing repweek2test4\n",
      "Processing repweek2test5\n",
      "1119 of 1119\r"
     ]
    }
   ],
   "source": [
    "for ID in rep_IDs:\n",
    "    print(f\"Processing {ID}\", end = \"\\n\")\n",
    "    write_img_inference(\n",
    "    predictor, \n",
    "    write_path = \"C:/R_projects/deep_learning_playground/inferences\",\n",
    "    rep_ID = ID,\n",
    "    read_path = \"C:/R_projects/spat_1f_noise/processed_feed\", \n",
    "    model_ver = \"modelv6\"\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183ae0c-812c-4443-bf2d-44ead40ff4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-detectron_env] *",
   "language": "python",
   "name": "conda-env-.conda-detectron_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
