{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c9ac50-09d3-4518-93f7-7cd57391e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:56:38_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n",
      "\n",
      "Torch CUDA version: 12.1\n",
      "\n",
      "Torch version: 2.1.1\n",
      "\n",
      "detectron2: 0.6\n",
      "\n",
      "cv2 version: 4.8.1\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2, cv2\n",
    "!nvcc --version\n",
    "print(\"\\nTorch CUDA version:\", torch.version.cuda)\n",
    "print(\"\\nTorch version:\", torch.__version__)\n",
    "print(\"\\ndetectron2:\", detectron2.__version__)\n",
    "print(\"\\ncv2 version:\",cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd57df5-e877-43f5-8417-0536c4fd7996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: \n",
      "Python 3.8.18\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version: \")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50920f3-30b3-41fd-b145-649646616d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor,DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.visualizer import ColorMode,GenericMask,Visualizer\n",
    "from detectron2.structures.keypoints import heatmaps_to_keypoints\n",
    "from detectron2.structures.keypoints_hack import heatmaps_to_keypoints2, heatmaps_to_keypoints_iterative, heatmaps_modify_iterative\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    DatasetEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    "    verify_results,\n",
    ")\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf00594-8f23-4c1b-b009-2ea4ffc964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\R_projects\\spat_1f_noise\\deep_learning\\detectron2\\custom_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8394b2-f1d0-4e07-87ee-3070e1c58f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63001ee9-b8f6-4467-8d3a-054f6fd614fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='cat_data_val',\n",
       "          json_file='.\\\\coco_val.json',\n",
       "          image_root='.\\\\datasets\\\\master_sample',\n",
       "          evaluator_type='coco',\n",
       "          thing_classes=['cat'],\n",
       "          thing_colors=(0, 0, 255),\n",
       "          keypoint_names=['head', 'middle', 'tail'],\n",
       "          keypoint_flip_map=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register datasets and populate metadata\n",
    "register_coco_instances(\"cat_data_train\", {}, \".\\coco_train.json\", \".\\datasets\\master_sample\")\n",
    "register_coco_instances(\"cat_data_test\", {}, \".\\coco_test.json\", \".\\datasets\\master_sample\")\n",
    "register_coco_instances(\"cat_data_val\", {}, \".\\coco_val.json\", \".\\datasets\\master_sample\")\n",
    "\n",
    "MetadataCatalog.get(\"cat_data_train\").set(thing_classes=[\"cat\"])\n",
    "MetadataCatalog.get(\"cat_data_train\").set(thing_colors=(0,0,255))\n",
    "MetadataCatalog.get(\"cat_data_train\").set(keypoint_names=[\"head\",\"middle\",\"tail\"])\n",
    "MetadataCatalog.get(\"cat_data_train\").set(keypoint_flip_map=[])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(thing_classes=[\"cat\"])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(thing_colors=(0,0,255))\n",
    "MetadataCatalog.get(\"cat_data_test\").set(keypoint_names=[\"head\",\"middle\",\"tail\"])\n",
    "MetadataCatalog.get(\"cat_data_test\").set(keypoint_flip_map=[])\n",
    "MetadataCatalog.get(\"cat_data_val\").set(thing_classes=[\"cat\"])\n",
    "MetadataCatalog.get(\"cat_data_val\").set(thing_colors=(0,0,255))\n",
    "MetadataCatalog.get(\"cat_data_val\").set(keypoint_names=[\"head\",\"middle\",\"tail\"])\n",
    "MetadataCatalog.get(\"cat_data_val\").set(keypoint_flip_map=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a57a2e-b071-45a0-8116-e00b994246fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d7da83-2d06-4f19-a8c5-5d4767d53900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom data maper for augmentation and custom validation data loss eval hook\n",
    "\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from detectron2.data import detection_utils as utils\n",
    "import detectron2.data.transforms as T\n",
    "import copy\n",
    "\n",
    "def custom_mapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    transform_list = [\n",
    "        T.Resize((1000,1000)),\n",
    "        T.RandomBrightness(0.8, 1.8),\n",
    "        T.RandomContrast(0.6, 1.3),\n",
    "        T.RandomSaturation(0.8, 1.4),\n",
    "        T.RandomRotation(angle=[90, 90]),\n",
    "        T.RandomLighting(0.7),\n",
    "        T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "    ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2], \n",
    "                                             keypoint_hflip_indices = [0,1,2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = instances\n",
    "    #dataset_dict[\"instances\"] = detectron2.data.detection_utils.filter_empty_instances(instances)\n",
    "    return dataset_dict\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer deriving from the \"DefaultTrainer\"\n",
    "\n",
    "    Overloads build_hooks to add a hook to calculate loss on the test set during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1, LossEvalHook(\n",
    "            200, # Frequency of calculation - every 200 iterations here\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                mapper=custom_mapper\n",
    "            )\n",
    "        ))\n",
    "\n",
    "        return hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa0596-8d92-4cf1-b5be-ddd8f8435a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b324ae2-5000-4056-a4d6-1fbe71836476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model configs\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"cat_data_train\",)\n",
    "cfg.DATASETS.TEST = (\"cat_data_val\", ) # Called test in detectron2, but is actually the validation data\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False \n",
    "cfg.TEST.EVAL_PERIOD = 0 # Turn off COCO evluator. \n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.0001  \n",
    "cfg.SOLVER.MAX_ITER = 12000    \n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\"\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.03 # Set to lower?\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 3\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.1, 0.3, 0.1]\n",
    "# cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.MODEL.MASK_ON = True\n",
    "cfg.MODEL.KEYPOINT_ON = True\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT = 5\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS = False\n",
    "#cfg.MODEL.ROI_KEYPOINT_HEAD.ITERATIVE = True\n",
    "cfg.INPUT.USE_DIFF = False                      # Include difference in green channel 10 frames ago as the 4th channel if True\n",
    "cfg.OUTPUT_DIR = \"./modelv7_1\"\n",
    "#cfg.MODEL.BACKBONE.FREEZE_AT = 0\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1000\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1000\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 1000\n",
    "\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 5000\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 3000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 2500\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 2000\n",
    "cfg.MODEL.RPN.NMS_THRESH = 0.7\n",
    "\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9070c7-48c5-4adc-834d-d7f2a9231914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: False\n",
      "  NUM_WORKERS: 2\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ('cat_data_val',)\n",
      "  TRAIN: ('cat_data_train',)\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1000\n",
      "  MAX_SIZE_TRAIN: 1000\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: 1000\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "  USE_DIFF: False\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32], [64], [128], [256], [512]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_fpn_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: True\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_ON: True\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
      "    FED_LOSS_NUM_CLASSES: 50\n",
      "    NAME: FastRCNNConvFCHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 2\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "    USE_FED_LOSS: False\n",
      "    USE_SIGMOID_CE: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 128\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: StandardROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 1\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.03\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    ITERATIVE: True\n",
      "    LOSS_WEIGHT: 5\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: False\n",
      "    NUM_KEYPOINTS: 3\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS: [-1]\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 2000\n",
      "    POST_NMS_TOPK_TRAIN: 2500\n",
      "    PRE_NMS_TOPK_TEST: 3000\n",
      "    PRE_NMS_TOPK_TRAIN: 5000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n",
      "OUTPUT_DIR: ./modelv7_1\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BASE_LR: 0.0001\n",
      "  BASE_LR_END: 0.0\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 500\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 2\n",
      "  LR_SCHEDULER_NAME: WarmupCosineLR\n",
      "  MAX_ITER: 12000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  NUM_DECAYS: 3\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  RESCALE_INTERVAL: False\n",
      "  STEPS: (210000, 250000)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: None\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 1\n",
      "  EVAL_PERIOD: 0\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: [0.1, 0.3, 0.1]\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043dd56c-56dd-499e-8b0f-8cc8e6af43c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0ee99-359f-4e9c-8150-01aa4d09cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6589df3-3870-43c7-bf44-af0a1e0889f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88454e6c-5124-4597-8c6c-8958e84a2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at model performance over iterations\n",
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir \"./modelv7_1\"\n",
    "\n",
    "### Go to directroy to delete temp file if TensorBoard fails to launch\n",
    "# !del /S C:\\Users\\vsbpa\\AppData\\Local\\Temp\\.tensorboard-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0080d-f0ce-43ef-8c35-16e32d7b2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fitted model as predictor\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_0007999.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # set to 0.3 for now, but actually filtered to 0.7 in R\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28ed2e-206b-4dab-b6d2-46315a5ec43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffa479-d462-4606-9854-97c53546b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For writing ground truths and predictions into a csv file that spat1f can process in R\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Function for writing csv\n",
    "def write_csv(path, df):\n",
    "    keys = df[0].keys()\n",
    "    with open(path, \"w\", newline = \"\") as output_file: \n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(df)\n",
    "\n",
    "# Extract predicted annotations\n",
    "def extract_pred_annotations(predictor, fi, data_name):\n",
    "    inst = predictor(cv2.imread(fi))[\"instances\"].to(\"cpu\")\n",
    "    data = {}\n",
    "    data[\"file_name\"] = fi\n",
    "    data[\"image_size\"] = list(inst.image_size)\n",
    "    try: data[\"thing_class\"] = MetadataCatalog.get(data_name).get(\"thing_classes\")[inst.pred_classes[0]]\n",
    "    except: data[\"thing_class\"] = \"NA\"\n",
    "    try: data[\"score\"] = inst.scores.tolist()[0]\n",
    "    except: data[\"score\"] = \"NA\"\n",
    "    try: data[\"keypoints\"] = np.array(inst.pred_keypoints.tolist()[0]).ravel().tolist()\n",
    "    except: data[\"keypoints\"] = \"NA\"\n",
    "    try: data[\"bbox\"] = inst.pred_boxes.tensor.tolist()[0]\n",
    "    except: data[\"bbox\"] = \"NA\"\n",
    "    try:\n",
    "        cont, _ = cv2.findContours(inst.pred_masks.numpy()[0,:,:].astype('uint8'),cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "        data[\"polygon\"] = cont[0].ravel().tolist()\n",
    "    except: \n",
    "        data[\"polygon\"] = \"NA\"\n",
    "    data[\"type\"] = \"prediction\"\n",
    "    return(data)\n",
    "\n",
    "# Extract ground truth annotations\n",
    "def extract_gt_annotations(data_dict, data_name):\n",
    "    data = {}\n",
    "    data[\"file_name\"] = data_dict[\"file_name\"]\n",
    "    data[\"image_size\"] = [data_dict[\"width\"], data_dict[\"height\"]]\n",
    "    \n",
    "    try: ann = data_dict[\"annotations\"][0] # One instance for now\n",
    "    except: ann = {}\n",
    "    try: data[\"thing_class\"] = MetadataCatalog.get('cat_data_test').get(\"thing_classes\")[ann[\"category_id\"]]\n",
    "    except: data[\"thing_class\"] = \"NA\"\n",
    "    data[\"score\"] = \"NA\"\n",
    "    try: data[\"keypoints\"] = ann[\"keypoints\"]\n",
    "    except: data[\"keypoints\"] = \"NA\"\n",
    "    try: data[\"bbox\"] = ann[\"bbox\"]\n",
    "    except: data[\"bbox\"] = \"NA\"\n",
    "    try: data[\"polygon\"] = ann[\"segmentation\"][0]\n",
    "    except: data[\"polygon\"] = \"NA\"\n",
    "    data[\"type\"] = \"ground_truth\"\n",
    "    return(data)\n",
    "    \n",
    "# Nice wrapper for prediction and ground truth data extraction and formatting\n",
    "def img_inference2(predictor, data_name, inference_info = \"\"):\n",
    "    d = DatasetCatalog.get(data_name)\n",
    "    dataset = []\n",
    "    tot = len(d)\n",
    "    for i in range(tot):\n",
    "        print(f\"{i+1} of {tot}\", end = \"\\r\")\n",
    "        fi = d[i][\"file_name\"]\n",
    "        data = extract_pred_annotations(predictor, fi, data_name)\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "        data = extract_gt_annotations(d[i], data_name)\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "    return(dataset)\n",
    "\n",
    "# Wrapper for applying img_inference2() to a registered dataset \n",
    "def write_img_inference2(predictor, write_path, file_name_ID, data_name, model_ver):\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    inference_info = model_ver + \"__\" + now\n",
    "    df = img_inference2(predictor, data_name, inference_info)\n",
    "    write_csv(os.path.join(write_path, file_name_ID + \"_inference.csv\"), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbc83b-8d2e-4801-83ec-56f35e0e4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For writing predictions\n",
    "\n",
    "# For performing model inference on all images in a subdirectory corresponding to the supplied rep_ID in a root directory\n",
    "def img_inference(predictor, rep_ID, root_path, inference_info):\n",
    "    dataset = []\n",
    "    f = glob.glob(os.path.join(root_path, rep_ID,\"*[!_diff].jpg\"))\n",
    "    tot = len(f)\n",
    "    for i in range(tot):\n",
    "        print(f\"{i+1} of {tot}\", end = \"\\r\")\n",
    "        fi = f[i]\n",
    "        inst = predictor(cv2.imread(fi))[\"instances\"].to(\"cpu\")\n",
    "        data = {}\n",
    "        data[\"file_name\"] = fi\n",
    "        data[\"image_size\"] = list(inst.image_size)\n",
    "        try: data[\"thing_class\"] = MetadataCatalog.get('cat_data_test').get(\"thing_classes\")[inst.pred_classes[0]]\n",
    "        except: data[\"thing_class\"] = \"NA\"\n",
    "        try: data[\"score\"] = inst.scores.tolist()[0]\n",
    "        except: data[\"score\"] = \"NA\"\n",
    "        try: data[\"keypoints\"] = np.array(inst.pred_keypoints.tolist()[0]).ravel().tolist()\n",
    "        except: data[\"keypoints\"] = \"NA\"\n",
    "        try: data[\"bbox\"] = inst.pred_boxes.tensor.tolist()[0]\n",
    "        except: data[\"bbox\"] = \"NA\"\n",
    "        try:\n",
    "            cont, _ = cv2.findContours(inst.pred_masks.numpy()[0,:,:].astype('uint8'),cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "            data[\"polygon\"] = cont[0].ravel().tolist()\n",
    "        except: \n",
    "            data[\"polygon\"] = \"NA\"\n",
    "        data[\"inference_info\"] = inference_info\n",
    "        dataset.append(data)\n",
    "    return(dataset)\n",
    "\n",
    "# Wrapper for writing model inference on all images in a subdirectory of the root directory \n",
    "def write_img_inference(predictor, write_path, rep_ID, read_path, model_ver):\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    inference_info = model_ver + \"__\" + now\n",
    "    df = img_inference(predictor, rep_ID, read_path, inference_info)\n",
    "    write_csv(os.path.join(write_path, rep_ID + \"_inference.csv\"), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143bc63-e509-43b4-bed4-4c1cad41715f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14453d-fa48-453c-9d7e-19648aa70700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887874f2-1fdc-4065-96b8-d10c0ec64c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the testing dataset, saving both groundtruths and predictions\n",
    "write_img_inference2(\n",
    "    predictor, \n",
    "    write_path = \"C:/R_projects/deep_learning_playground\",\n",
    "    file_name_ID = \"master_modelv7_1_7999iter_test\",\n",
    "    data_name = \"cat_data_test\", \n",
    "    model_ver = \"modelv7.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7281ebe-d22b-465d-9737-bf1ab2916707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbb511-edc2-4e57-86cb-b5141408319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8fde-a3c4-46b7-a4db-b02b1e49754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the training dataset, saving both groundtruths and predictions\n",
    "write_img_inference2(\n",
    "    predictor, \n",
    "    write_path = \"C:/R_projects/deep_learning_playground\",\n",
    "    file_name_ID = \"master_modelv7_1_7999iter_train\",\n",
    "    data_name = \"cat_data_train\", \n",
    "    model_ver = \"modelv7.1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91893ae-7689-45df-a2f2-0099fbb84d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f232039-2882-4c85-80ab-574af442c4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13b996-4d7c-40cc-a27d-d49dcadd8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of rep_IDs for looping\n",
    "rep_IDs = [os.path.basename(x) \n",
    " for x in \n",
    " glob.glob(\"C:/R_projects/spat_1f_noise/processed_feed/rep*\", recursive=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f5479-af8c-4dc5-94d9-4d33b3d39226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model inference on all rep_IDs\n",
    "for ID in rep_IDs:\n",
    "    print(f\"Processing {ID}\", end = \"\\n\")\n",
    "    write_img_inference(\n",
    "    predictor, \n",
    "    write_path = \"C:/R_projects/deep_learning_playground/inferences\",\n",
    "    rep_ID = ID,\n",
    "    read_path = \"C:/R_projects/spat_1f_noise/processed_feed\", \n",
    "    model_ver = \"modelv7.1\"\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183ae0c-812c-4443-bf2d-44ead40ff4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-detectron_env] *",
   "language": "python",
   "name": "conda-env-.conda-detectron_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
